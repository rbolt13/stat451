---
title: "Chapter 4 : Continuous Random Variables and Probability Distibutions"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'chapter4.html'))})
output: 
  html_document: rmdformats::readthedown
---

# Definitions

> Let $X$ be a continuous random variable. Then a **probability distribution** or **probability density function** (pdf) of $X$ is a funciton $f(x)$ such that any two numbers a and b with $a\leq b$, $$P(\alpha \leq X \leq b) = \int_{a}^{b} f(x) dx$$ That is, the probability that $X$ takes on a value in the interval $[a,b]$ is the area above this interval and under the graph of the density funciton, as illustrated in **Figure 4.2** . The graph of $f(x)$ is often referred to as the *density curve*.  
<center>
![](img/fig4.2.png)
</center>

> A continuous random variable $X$ is said to have a **uniform distribution** on the interval $[A,B]$ if the pdf of $X$ is <center>
\[ 
f(x;A,B) = \begin{cases}
      \frac{1}{B-A} & A \leq x \leq B \\
      0 & \text{otherwise} 
   \end{cases}
\]
![](img/fig4.6and7.png)
</center>

* continuous at all possible values of interval

* cdf of x is : 
\[ 
F(X) = \begin{cases}
      0 & if x\leq A \\
      \frac{x-A}{B-A} & if A\leq x\leq B\\ 
      1 & x \geq B
   \end{cases}
\]

* cdf is always a non-decreasing function 

> The **cumulative distribution function** $F(x)$ for a continuous random variable $X$ is defined for ever number $x$ by $$F(x) = P(X \leq x ) \ \int_{- \infty}^{x} f(y)dy $$ For each $x, F(x)$ is the area under the density curve to the left of x. This is illustrated in **Figure 4.5**, where $F(x)$ increases smoothly as x increases. 
<center>
![](img/fig4.5.png)
</center>

> Let $p$ be a number between 0 and 1. The $(100p)$th **percentile** of the distribution of a continuous rv X, denoted by $\eta (p)$, is defined by $$p=F(\eta (p))=\int_{-\infty}^{\eta (p)}f(y)dy$$
<center>
![](img/fig4.10.png)
</center>

* For $\eta =\frac{1}{2}$ the median is $\eta (p)$

> The **median** of a continous distribution, denoterd by $\widetilde \mu$ , is the 50th percentile, so $\widetilde \mu$ satisifies $0.5=F(\widetilde \mu)$ . That is, half the area under the density curve is to the left of $\widetilde \mu$ and half is to the right of $\widetilde \mu$ . 

<center>
> ![](img/fig4.12.png)
</center>

> The **expected** or **mean value** of a continuous rv X with pdf $f(x)$ is $$\mu x=E(x)=\int_{-\infty}^{\infty}x\cdot f(x)dx$$

> The **variance** of a continuous rv X with pdf $f(x)$ and mean value $\mu$ is $$\sigma_{X}^{2}=V(X)=\int_{-\infty}^{\infty}(x-\mu)^2\cdot f(x)dx=E[(X-\mu)^2]$$

> The **standard deviation** (SD) of X is $\sigma_{X}=\sqrt{V(X)}$

> A continuous rv X is said to have a **normal distribution** with parameters $\mu$ and $\sigma$ (or $\mu$ and $\sigma ^2$), where$-\infty < \mu < \infty$ and $0<\infty$, if the pdf of X is $$f(x;\mu ,\sigma )=\frac{1}{\sqrt{2\pi }\sigma }e^{\frac{-(x-\mu)^2}{{(2\sigma ^2)}}}\quad{-\infty <x<\infty}$$ 

* Gaussian Distribution , bell shape curve

* most popular distribution, and most distributions are related to this model

* $\mu$ : mean is the center of the curve, and $\sigma$ is the vairablity 

> The normal distribution with parameter values $\mu = 0$ and $\sigma =1$ is called the **standard normal distribution**. A rv having a standard normal distribution is called a **standard normal random variable** and will be denoted by $Z$. the pdf of $Z$ is $$f(x;0,1)=\frac{1}{\sqrt{2\pi }}e^{\frac{-z^2}{2}}\quad{-\infty <x<\infty}$$
> The graph of $f(z;0,1)$ is called the *standard normal* (or $\mathscr{z}$) curve. Its inflections points are at 1 and -1. The cdf of Z is $P(Z\leq z)=\int_{-\infty}^{z}f(y;0,1)dy$ , which we will denote by $\Phi (\mathscr{z})$ .
<center>
![](img/fig4.14.png)
</center>

* When z goes to infinity the e^ goes to 0 very fast 

* z table goes from z-values of -3.4 to 3.4 

* E[Z]=0, and Var[Z]=1

* For **any** distribution $z=\frac{x-\mu}{\sigma}\approx N(0,1)$

* ![](img/fig4.16.png)

> A **continuity correction** is an adjustment that is made when a discrete distribution is approximated by a continuos distribution. 

* usually adding or subtracting 0.5 before standardizing 

> $X$ is said to have an **exponential distribution** with (scale) parameter $\lambda (\lambda >0)$ if the pdf of $X$ is
\[ 
f(x;\lambda ) = \begin{cases}
      \lambda e^{-\lambda x} & x\geq 0 \\
      0 &\text{otherwise}\
   \end{cases}
\]
<center>
![](img/fig4.26.png)
<\center>


* With cdf :
\[ 
F(x;\lambda ) = \begin{cases}
      0 & x< 0 \\
      1-e^{-\lambda x} &x\geq 0\
   \end{cases}
\]

* pdf given in class : 
\[ f(x) = \begin{cases}
      0 & x\leq 0 \\
      \frac{1}{\mu}e^{-x/\mu} & x\geq 0\
   \end{cases}\]

* cdf given in class $F(x)=1-e^{-x/\mu}$ if $x>0$

* $E(X)=\mu$

* $Var(X)=\sigma (x)=\mu$

* Exponential distributions are used to model lifetimes 


> For $\alpha >0$, the **gamma function** $\Gamma (\alpha)$ is defined by $$\Gamma (\alpha)=\int_{0}^{\infty} x^{\alpha -1}e^{-x}dx$$

The most important properties of the gamma function are thus: 

* For any $\alpha >1$, $\Gamma (\alpha) = (\alpha -1)\cdot \Gamma (\alpha -1)$ [via integration by parts]

* For any positive interger $n$, $\Gamma (n)=(n-1)!$

* $\Gamma (\frac{1}{2})=\sqrt{\pi}$

pdf of a Gamma function: 
\[ 
f(x;\alpha ) = \begin{cases}
      \frac{x^{\alpha-1}e^{-x}}{\Gamma (\alpha)} & x\geq 0 \\
      0 &\text{otherwise}\
   \end{cases}
\]

* $\Gamma (\alpha)=\int_{0}^{\infty} x^{\alpha -1}e^{-x}dx=(\alpha -1)\int_{0}^{\infty} x^{\alpha -2}e^{-x}dx$

* If $\alpha$ is an (enliga?) then $$\sqrt{(\alpha)}=(\alpha -1)!$$ $$\sqrt{(\frac{1}{2})}=\sqrt{(\pi)}$$

> A continuous random variable $X$ is said to have a **gamma distribution** if the pdf of X is 
\[ 
f(x;\alpha ,\beta) = \begin{cases}
      \frac{x^{\alpha-1}e^{\frac{-x}{\beta}}}{\beta ^{\alpha }\Gamma (\alpha)} & x\geq 0 \\
      0 &\text{otherwise}\
   \end{cases}
\]
> where the parameters $\alpha$ and $\beta$ satisfy $\alpha > 0$, $\beta >0$. The **standard gamma distribution** has $\beta =1$, so the pdf of a standard gamma rv is given **Fig.4.27**. 
<center>
![](img/fig4.27.png)
</center>

* $E(X)=\mu =\alpha \beta$

* $V(X)=\sigma ^2=\alpha \beta ^2$

cdf of X for standard gamma rv 

* $F(x;\alpha )=\int_{0}^{x}\frac{y^{\alpha-1}e^{-y}}{\Gamma (\alpha)}dy \quad x>0$

Note that an **incomplete gamma function** doesn't have a $\Gamma (\alpha)$ in the integral

> Let $\nu$ be a positive integer. Then a random variable $X$ is said to have a **chi-squared distribution** with parameter $v$ if the pdf of $X$ is that gamma denstiy with $\alpha = \frac{v}{2}$ and $\beta =2$. The pdf of a chi-squared rv is thus
\[ 
f(x;\nu) = \begin{cases}
      \frac{x^{(\nu /2)-1}e^{-x/2}}{2^{\nu /2}\Gamma (\frac{\nu}{2})} & x\geq 0 \\
      0 &x<0\
   \end{cases}
\]
The parametr $\nu$ is called the **number of degrees of freedom** (df) of $X$. The symbol $\chi ^2$ is oftern used in place of "chi-squared". 

> A random variable X is said to have a **Weilbull distribution** with shape parameter $\alpha$ and scale parameter $\beta (\alpha >0,\beta >0)$ if the pdf of $X$ is 
\[
f(x;\alpha ,\beta ) = \begin{cases}
      \frac{\alpha }{\beta ^\alpha }x^{\alpha -1}e^{-(x/\beta )^\alpha} & x\geq 0 \\
      0 & x < 0\
      \end{cases}
\]
<center>
![](img/fig4.28.png)
<\center>

* the cdf of the Weilbull rv having parameters $\alpha$ and $\beta$ is 
\[
F(x;\alpha ,\beta ) = \begin{cases}
         0 & x<0\\
         1-e^{-(x/\beta )^\alpha} & x\geq 0\
         \end{cases}
\]

> A nonegative rv $X$ is said to have **lognormal distribution** if the rv $Y=ln(X)$ has a normal distribution. The resulting pdf of a lognormal rv when $ln(X)$ is normally distributed with parameters $\mu$ and $\sigma$ is 
\[
f(x;\mu ,\sigma ) = \begin{cases}
         \frac{1}{\sqrt{2\pi}\sigma x}e^{-[ln(x)-\mu]^{2}/(2\sigma ^2)} & x\geq0\\
         0 & x<0\
         \end{cases}
\]
<center>
![](img/fig4.30.png)
<\center>

* Because $ln(X)$ has a normal distribution, the cdf of $X$ can be expressed in terms of the cdf $\phi (z)$ of a standard normal rv $Z$. \begin{equation} \label{eqlog}
\begin{split}
F(x;\mu \sigma) & = P(X\leq x)=P[ln(X)\leq (x)]\\
& = P(Z\leq \frac{ln(x)-\mu }{\sigma})=\Phi (\frac{ln(x)-\mu }{\sigma}) \quad x\geq 0
\end{split}
\end{equation}

* be careful!! $\sigma$ and $\mu$ are not the mean and standard deviation of $X$ but of $ln(X)$

* $E(X)=e^{\mu +\sigma ^2 /2}$

* $V(X)=e^{2\mu +\sigma ^2}\cdot (e^{\sigma ^2}-1)$

> A random variable $X$ is said to have a **beta distribution** with parameters $\alpha$, $\beta$ (both postive), $A$, and $B$ if the pdf of $X$ is 
\[
f(x;\alpha ,\beta ,A, B) = \begin{cases}
         \frac{1}{B-A}\cdot\frac{\Gamma (\alpha +\beta )}{\Gamma (\alpha )\cdot \Gamma (\beta )}(\frac{x-A}{B-A})^{\alpha -1}(\frac{B-x}{B-A})^{\beta -1} & A \leq x\leq B\\
         0 & \text{otherwise}\
         \end{cases}
\]
> The case $A=0$ , $B=1$ give the **standard beta distribution** .
<center>
![](img/fig4.32.png)
<\center>

> A **probability plot** is used for assessing whether or not a data set follows a given distribution (such as normal or Weibull). 

> Order the $n$ sample observations from the smallest to largest. Then the $ith$ smallest observation in the list is taken to be the $[100(i-0.5)/n]$ th **sample percentile** 

> A plot of the $n$ pairs $$[(100(i-0.5)/n]\text{th z percentile, ith smallest observation})$$ is called a **normal probability plot**. If the sample observations are in fact drawn from a normal distribution with mean value $\mu$ and standard deviation $\sigma$, the points should fall close to a straight line with slope $\sigma$ and intercept $\mu$ . Thus a plot for which the points call close to some straight line suggests that the assumption of a normal population distribution is plausible. 

* A non normal population distribution can oftern be placed in one of the following three categories: 

1. It is symmetric and has "lighter tails" than does a normal distribution; that is the density curve declines more rapidly out in the tails than does a normal curve. (uniform distribution)

2. It is symmetric and heavy-tailed compared to a normal distribution. (S-shaped plot, with left endpoint downward : observed < z percentile )

3. It is skewed. 

* $\theta _{1}=\mu \quad \theta _{2}=\sigma$  are location and scale parameters respectively. 

# Theorems

> **Proposition** : Let $X$ be a continuous random variable with pdf $f(x)$ and cdf $F(x)$. Then for any number $a$, $$P(X>a)=1-F(a)$$ and for any two numbers a and b with $a<b$, $$P(a \leq X \leq b) = F(b)-F(a)$$
<center>
![](img/fig4.8.png)
</center>

> **Proposition - Obtaining f(x) from F(x) :** IF X is a continuos rv with pdf $f(x)$ and cdf $F(x)$, then at every x at which the derivative $F'(x)$ exists, $F'(x)=f(x)$ .

* $f(x)=\frac{d}{dx}F(x)$ : find pdf by differentiating cdf

* $F(x)=\int_{-\infty}^{x}f(t)dt$ : find cdf by integrating pdf

* x is fixed point, do not use x within the function that being integrated (use dummy variable t)

> **Proposition :** If X is a continous rv with pdf $f(x)$ and $h(X)$ is any function of X, then $$E[h(X)]=\mu_{h}(x)=\int_{-\infty}^{\infty}h(x)\cdot f(x)dx$$

> **Proposition :** $$V(X)=E(X^2)-[E(X)]^2$$

* Special case is $U[0,1]$ where $E(X)=\frac{1}{2}$ , $Var(X)\frac{1}{12}$

> **Proposition - Nonstandard Normal Distributions :** If $X$ has a normal distribution with mean $\mu$ and standard deviation $\sigma$, then $$Z=\frac{X-\mu}{\sigma}$$ has a standard normal distribution. Thus 
\begin{equation} \label{eqa}
\begin{split}
P(a\leq X\leq b) & =P(\frac{a-\mu}{\sigma}\leq Z\leq \frac{b-\mu}{\sigma})\\
& =\Phi (\frac{b-\mu}{\sigma})-\Phi (\frac{a-\mu}{\sigma})
\end{split}
\end{equation}
$P(X\leq a) = \Phi (\frac{a-\mu}{\sigma}) \quad \quad P(X\geq b)=1-\Phi(\frac{a-\mu}{\sigma})$

<center>
* ![](img/fig4.21.png)
</center>

* If the population distribution of a variable is (approximately) normal, then : 

1. Roughly 68% oft he values are within 1 SD of the mean.
2. Roughly 95% oft he values are within 2 SD of the mean.
3. Roughly 99.7% oft he values are within 3 SD of the mean.

> **Proposition - Percentiles of an Arbitrary Normal Distribution :** $$(100p)\text{th percentile for normal} (\mu , \sigma ) = \mu +[(100p)\text{th for stadard normal}]\cdot \sigma$$

> **Proposition :** Let $X$ be a binomial rv based on $n$ trials with sccess probability $p$. Then if the binomial probability histogram is not too skewed, $X$ has approximately a normal distribution with $\mu=n p$ and $\sigma =\sqrt{npq}$. In particular, for $x=\text{a possible value of X}$,$$P(X\leq x)=B(x,n , p)\approx {\text{are under the normal curve}\choose{\text{to the left of}\quad x+0.5}}=\Phi(\frac{z+0.5-np}{\sqrt{npq}})$$ In practice, the approximation is adequate provided that both $np\geq10$ and $nq\geq10$ (i.e. the expected number of successes and the expected number of failures are both at least 10), since there is then enough symmetry in the underlying binomial distribution. 

> **Proposition :** Suppose that the number of events occuring in any time interval of length t has a Poisson distribution with parameter $\alpha t$ (where $\alpha$ , the rate of the event process, is the expected number of events occuring in 1 unit of time) and that numbers of occurrences in nonoverlapping intervals are independednt of one another. Then the distribution of elapsed time between the occurrence of two successive events is exponential with parameter $\lambda =\alpha$

> **Proposition :** Let $X$ have a gamma distribution with parameters $\alpha$ and $\beta$. Then for any $x>0$, the cdf of $X$ is given by $$P(X\geq x)=F(x;\alpha, \beta)=F(\frac{x}{\beta};\alpha)$$ where $F(\cdot ;\alpha)$ is the incomplete gamma function.

# Examples

### Exercise 1 (page 146)
**The current in a certain circuit as measured by an ammeter is a continuous random variable X with the following density function:** 
\[ 
f(x) = \begin{cases}
      0.075x +0.2 & 3\leq x\leq 5 \\
      0 &\text{otherwise}\
   \end{cases}
\]

a. **Graph the pdf and verify that the total area under the density curve is indeed 1.** 
```{r, echo=F}
curve(0.075*x+0.2,3,5, ylim = c(0,0.75), ylab = "y")
```
\begin{equation} \label{eq1}
\begin{split}
\int_{-\infty}^{\infty}f(x)dx & =\int_{-\infty}^{3}0+\int_{3}^{5}(0.075x+0.2)dx+\int_{5}^{\infty}0\\
& =\frac{0.075x^2}{2}|_{3}^{3}+0.2(5-3)\\
& =\frac{(0.75)5^2}{2}-\frac{(0.75)3^2}{2}+0.4\\
& =1
\end{split}
\end{equation}

b. **Calculate $P(X\leq 4)$. How does this probability compare to P($X<4$)?**

\begin{equation} \label{eq2}
\begin{split}
P(X\leq 4) & =\int_{-\infty}^{4}f(x)dx\\
& =0+\int_{3}^{4}(0.70x+0.2)dx\\
& =\frac{0.075x^2}{2}+0.2|_{3}^{4}\\
& =0.4625
\end{split}
\end{equation}

$P(X\leq 4)=P(X<4)$

c. **Calculate $P(3.5\leq X\leq 4.5)$ and also P($4.5<X$).**

\begin{equation} \label{eq3}
\begin{split}
P(3.5\leq X\leq 4.5) & =\int_{3.5}^{4.5}f(x)dx\\
& =0.075\frac{x^2}{2}|_{3.5}^{4.5}+0.2\\
& =0.075(\frac{4.5^2}{2}-\frac{3.5^2}{2})+0.2\\
& =0.075(10.125-6.125)+0.2\\
& =0.5
\end{split}
\end{equation}

$P(X<4.5)=0.278125$
 
### Exercise 13 (page 155)
**Example 4.5** introduced the concept of time headway in traffic flow and proposed a particular distribution for X = the headway between two randomly selected consecutive cars (sec). Suppose that in a different traffic environment, the distribution of time headway has the form
\[ 
f(x) = \begin{cases}
      \frac{k}{x^4} & x>1\\
      0 & x\leq 1
   \end{cases}
\]
(a.) **Determine the value of $k$ for which $f(x)$ is a legitimate pdf.**
 
What is the value of k>0?
\begin{equation} \label{eq4}
\begin{split}
1 =\int_{-\infty}^{\infty}f(x)dx & =\int_{-\infty}^{1}f(x)dx+\int_{1}^{\infty}f(x)dx\\
& = 0+k\int_{1}^{\infty}\frac{1}{x^4}dx\\
& = k(\frac{x^{-3}}{-3})|_{1}^{\infty}\\
& = \frac{k}{3}
\end{split}
\end{equation}
$\Rightarrow k = 3$
 
(b.) **Obtain the cumulative distribuion function.** 

 $F(x)=0$ if $x\leq 1$ . So if $x>1$
 \begin{equation} \label{eq5}
\begin{split}
F(x) & =\int_{-\infty}^{x}f(t)dt \pm \int_{-\infty}^{1}f(t)dt + \int_{1}^{x}f(t)dt\\
& = 0 + \int_{1}^{x}\frac{3}{t^4}dt\\
& = 3(\frac{t^{-3}}{-3}|_{1}^{x})\\
& = 1 - \frac{1}{x^3}
\end{split}
\end{equation}
Therefore,
\[ 
F(x) = \begin{cases}
      0 & x\leq 1\\
      1 - \frac{1}{x^3} & x>1
   \end{cases}
\]
 
(c.) **Use the cdf from (b) to determine the probability that headway exceeds 2 sec and also the probability that headway is between 2 and 3 sec.** 
 
\begin{equation} \label{eq6}
\begin{split}
P[X>2] & =1-P[x\leq 2]\\
& = 1-F(2)\\
& = 1-(1-\frac{1}{2^3})\\
& = \frac{1}{8}
\end{split}
\end{equation}
\begin{equation} \label{eq7}
\begin{split}
P[2\leq x\leq 3] & =F(3)-F(2)\\
& = 1-\frac{1}{3^3}-(1-\frac{1}{2^3})\\
& = \frac{1}{8}-\frac{1}{27}\\
& = \frac{19}{216}
\end{split}
\end{equation}
 
(d.) **Obtain the mean value of headway and the standard deviation of headway** 

![](img/d.png) 
 
(e.) **What is the probability that headway is within 1 standard deviation of the mean value?** 
 
\begin{equation} \label{eq9}
\begin{split}
P[\mu -\sigma \leq x \leq \mu +\sigma] & =P[1.5-0.866\leq x \leq 1.5+0.866]\\
& =P[0.34\leq x \leq 2.366]\\
& = F(2.366)-F(0.634)\\
& = 924
\end{split}
\end{equation}
 
### In Class Example (July 22nd 2021)
For exponential distribution with mean $\mu$, to find the median, solve the equations for $F(X)=\frac{1}{2}$


$$1-e^{-x/\mu}  =\frac{1}{2}$$
$$e^{-x/\mu}  =\frac{1}{2}$$
$$\frac{-x}{\mu}=ln(\frac{1}{2})$$
$$\Rightarrow M=-\mu ln(\frac{1}{2})=\mu ln(2)$$ 

* skewed to the right, drops exponentially fast to zero 

Suppose x is the lifetime, and s is when the item stops working (when x>s). What is the conditional probability that it will live longer than the expected value. $P[x>s+t|x>s]$

\begin{equation} \label{eq8}
\begin{split}
P[x>s+t|x>s] & =\frac{P[x>s+t]}{P[x>s]}\\
& = \frac{1-F(s+t)}{1-F(s)}\\
& = \frac{e^{-(s+t)}}{\mu}\\
& = e^{-t/\mu}
\end{split}
\end{equation}

* independent of s! called the memory-less property (forgets how long its been working)