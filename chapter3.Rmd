---
title: "Chapter 3 : Discrete Random Variables and Probability Distribution"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'chapter3.html'))})
output: 
  html_document:
    css: css/statCSSBackground.css
    theme: readable
---

## Definitions

> For a given sample space $\mathscr{S}$ of some experiment, a **random variable (rv)** is any rule that associates a number with each outcome in $\mathscr{S}$. In mathematical language, a random variable is a funciton whose domain is the sample space and whose range is the set of real numbers. 

* Random variables are denoted by uppercase letters such as $X$ and $Y$. 

> Any random variable whose only possible values are 0 and 1 is called a **Bernoulli random variable** . 

> A **discrete** random variable is an rv whose possible values either consitiute a finite set or else can be listed in an infinite sequence in which there is a first element, a second element, and so on ("countable" infinite).

> A random variable is **continuous** if *both* of the following apply:
1. Its set of possible values consists either of all numbers in a single interval on the number line ($-\infty$ to $\infty$) or all number in a disjoint union of such intervals ($[0,10] \cap [20, 30]$). 

> The **Probability distribution** or **probability mass function** (pmf) of a discrete rv is defined for every number $x$ by $p(x)=P(X=x)=P($all $\omega \in \mathscr{S}:X(\omega)=x)$.

> Suppose $p(x)$ depends on a quantity that can be assigned any one of a number of possible values, with each different value determining a different probability distribution. Such a quantity is called a **parameter** of the distribution. The collection of all probability distributions for different value of the parameter is called a **family** of probability distributions. 

* Sometimes expressed $p(x;\alpha)$, where $\alpha$ is the parameter

* The Bernoulli family of distributions is between 0 and 1. 

> The **cumulative distribution** (cdf) $F(x)$ of a discrete random variable $X$ with pmf p(x) is defined for every number x by $$F(x)=P(X \leq x)=\sum_{y:y \leq x}^{} p(y)$$ For any number $x$, $F(x)$ is the probability that the observed value of $X$ will be at most x. 

> Let $X$ be a discrete random variable of possible values D and pmf $p(x)$. The **expected value** or **mean value** of $X$ denoted by $E(X)$ of $\mu x$ or just $\mu$, is $$E(X)=\mu x = \sum_{x \in D}^{} x\cdot p(x)$$

> Let $X$ have pmf p(x) and expected value $\mu$. Then the **variance** of $X$, denoted by $V(X)$ or $\sigma_{X}^2$ or just $\sigma ^2$, is $$V(X)=\sum_{D}^{} (x-\mu)^2 \cdot p(x)=E(X-\mu)^2]$$ The **standard deviation** (SD) of X is $$\sigma x \sqrt{ \sigma_{X}^2}$$ 


## Theorems

> **Proposition - Computing Probabilities :** For any two numbers $a$ and $b$ with $a \leq b$, $$ P(a \leq X \leq b)=F(b)-F(a-)$$ where "$a-$" represents the largest possible X value that is strickly less than $a$. In particular, if the only possible values are integers and $a$ and $b$ are integers, then $$P(a \leq X \leq b)=P(X=a or a+1 or ..... or b) = F(b)-F(a-1)$$ Taking $a=b$ yields $P(X=a)=F(a)-F(a-1)$ in this case. 

* mainly used for binomial and Poisson probabilities

> **Proposition - Expected Value of a Function : **If the random variable $X$ has a set of possible values D and pmf $p(x)$, then the expected value of any function $h(X)$, denothed by $E[h(X)]$ or $\mu_{h(x)}$ is computed by $$E[h(X)]=\sum_{D}^{} h(x) \cdot p(x)$$


> **Proposition - Expected Value of a Linear Function : **$$E(aX+b)=a\cdot E(X)+b$$ (Or, using alternative notation, $\mu_aX+b = a\cdot \mu x +b$). To paraphrase, the expected value of a linear function equals the linear function evaluated at the expected value $E(X)$. 

Two important rules of expected values :

1. For any constant $a,E(aX)=a\cdot E(x)$ (take $b=0$).

2. For any constant $b,E(bx)=E(X)+b$ (take $a = 1$).

> **Proposition - Shortcut for $\sigma^2$ : ** $$V(X)=\sigma^2=[\sum_{D} x^2\cdot p(x)]-\mu^2=E(X^2)-[E(X)]^2$$

> **Proposition - Variance of a Linear Function : **$$V(aX)+b=\sigma_{aX+b}^2 = a^2 \cdot \sigma_{X}^{2}$$ and $$\sigma_{aX+b} = |a| \cdot \sigma_X$$ In particular, $$\sigma_{aX} = |a| \cdot \sigma X, \sigma_{X+b}=\sigma X$$

