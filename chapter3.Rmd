---
title: "Chapter 3 : Discrete Random Variables and Probability Distribution"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'chapter3.html'))})
output: 
  html_document: rmdformats::readthedown
---
```{r, echo=F, message=F}
library(dplyr)
library(kableExtra) # highlights table when hover over a line
```

## Definitions

> For a given sample space $\mathscr{S}$ of some experiment, a **random variable (rv)** is any rule that associates a number with each outcome in $\mathscr{S}$. In mathematical language, a random variable is a funciton whose domain is the sample space and whose range is the set of real numbers. 

* Random variables are denoted by uppercase letters such as $X$ and $Y$. 

> Any random variable whose only possible values are 0 and 1 is called a **Bernoulli random variable** . 

> A **discrete** random variable is an rv whose possible values either consitiute a finite set or else can be listed in an infinite sequence in which there is a first element, a second element, and so on ("countable" infinite).

> A random variable is **continuous** if *both* of the following apply:
1. Its set of possible values consists either of all numbers in a single interval on the number line ($-\infty$ to $\infty$) or all number in a disjoint union of such intervals ($[0,10] \cap [20, 30]$). 

> The **Probability distribution** or **probability mass function** (pmf) of a discrete rv is defined for every number $x$ by $p(x)=P(X=x)=P($all $\omega \in \mathscr{S}:X(\omega)=x)$.

> Suppose $p(x)$ depends on a quantity that can be assigned any one of a number of possible values, with each different value determining a different probability distribution. Such a quantity is called a **parameter** of the distribution. The collection of all probability distributions for different value of the parameter is called a **family** of probability distributions. 

* Sometimes expressed $p(x;\alpha)$, where $\alpha$ is the parameter

* The Bernoulli family of distributions is between 0 and 1. 

> The **cumulative distribution** (cdf) $F(x)$ of a discrete random variable $X$ with pmf p(x) is defined for every number x by $$F(x)=P(X \leq x)=\sum_{y:y \leq x}^{} p(y)$$ For any number $x$, $F(x)$ is the probability that the observed value of $X$ will be at most x. 

> Let $X$ be a discrete random variable of possible values D and pmf $p(x)$. The **expected value** or **mean value** of $X$ denoted by $E(X)$ of $\mu x$ or just $\mu$, is $$E(X)=\mu x = \sum_{x \in D}^{} x\cdot p(x)$$

> Let $X$ have pmf p(x) and expected value $\mu$. Then the **variance** of $X$, denoted by $V(X)$ or $\sigma_{X}^2$ or just $\sigma ^2$, is $$V(X)=\sum_{D}^{} (x-\mu)^2 \cdot p(x)=E(X-\mu)^2]$$ The **standard deviation** (SD) of X is $$\sigma x \sqrt{ \sigma_{X}^2}$$ 

> An experiment for which **Condition 1**, **Condition 2**, *Condition 3**, **Condition 4** (a fixed number of dichotomous, independent, homogenous trials) are satisfied is called a **binomial experiment**.  

> The **binomial random variable X** associated with a binomial experiment consisting of $n$ trials is defined as $X =$ the number of S's among the n trials.

* because the pmf of a binomial random variable X depends on the two parameters $n$ and $p$, we denote the pmf by $b(x;n,p)$

## Theorems

> **Proposition - Computing Probabilities :** For any two numbers $a$ and $b$ with $a \leq b$, $$ P(a \leq X \leq b)=F(b)-F(a-)$$ where "$a-$" represents the largest possible X value that is strickly less than $a$. In particular, if the only possible values are integers and $a$ and $b$ are integers, then $$P(a \leq X \leq b)=P(X=a or a+1 or ..... or b) = F(b)-F(a-1)$$ Taking $a=b$ yields $P(X=a)=F(a)-F(a-1)$ in this case. 

* mainly used for binomial and Poisson probabilities

> **Proposition - Expected Value of a Function : **If the random variable $X$ has a set of possible values D and pmf $p(x)$, then the expected value of any function $h(X)$, denothed by $E[h(X)]$ or $\mu_{h(x)}$ is computed by $$E[h(X)]=\sum_{D}^{} h(x) \cdot p(x)$$


> **Proposition - Expected Value of a Linear Function : **$$E(aX+b)=a\cdot E(X)+b$$ (Or, using alternative notation, $\mu_aX+b = a\cdot \mu x +b$). To paraphrase, the expected value of a linear function equals the linear function evaluated at the expected value $E(X)$. 

Two important rules of expected values :

1. For any constant $a,E(aX)=a\cdot E(x)$ (take $b=0$).

2. For any constant $b,E(bx)=E(X)+b$ (take $a = 1$).

> **Proposition - Shortcut for $\sigma^2$ : ** $$V(X)=\sigma^2=[\sum_{D} x^2\cdot p(x)]-\mu^2=E(X^2)-[E(X)]^2$$

> **Proposition - Variance of a Linear Function : **$$V(aX)+b=\sigma_{aX+b}^2 = a^2 \cdot \sigma_{X}^{2}$$ and $$\sigma_{aX+b} = |a| \cdot \sigma_X$$ In particular, $$\sigma_{aX} = |a| \cdot \sigma X, \sigma_{X+b}=\sigma X$$

> **Rule - binomial "without-replacement" : **Consider sampling without replacement from a dichotomous population of size N. if the sample size (number of trials) $n$ is at most 5% of the population size, the experiment can be analyzed as though it were a binomial experiment. 

* dichotomous : divided into two branches 

> **Theorem - Binomial Probability :**\usepackage{amsmath}
\[ 
b(x;n,p) = \begin{cases}
      (\frac{n}{p})p^x(1-p)^{n-x} & x =0,1,2,...n \\
      0 &\text{otherwise}\
   \end{cases}
\]
In otherwords: 
\[ 
b(x;n,p) = \begin{cases}
      \text{number of sequences of} \\
      \text{length n consisting of x S's}\
   \end{cases} \cdot \begin{cases}
      \text{probability of any} \\
      \text{particular such sequence}\
   \end{cases}
\]

> **Proposition - The Mean and Variance of X :**If $X$~Bin$(n,p)$, then $E(X)=np$, $V(X)=np(1-p)=npq$, and $\sigma x=\sqrt{npq}$ (where $q=1-p$)

> **Proposition - Hypergeometric distribution : **If $X$ is the number of S's (successes) in a completely random sample of size $n$ drawn from a population consisting of M S's and $(N-M)$F's (failures), then the probability distribution of $X$, called the **hypergeometric distribution**, is given: $$P(X=x)=h(x;n,M,N)=\frac{{M\choose x}{N-M\choose n-x}}{{N\choose n}}$$ for x an integer satisfying max$(o,n-N+M)\leq x\leq min(n,M)$.

> **Proposition - Mean and Variance of Hypergeometric Distribution : **The mean and variance of the hypergeometric rv $X$ having pmf $h(x;n,M,N)$ are $$E(x)=n\cdot\frac{M}{N}$$  $$V(X)=(\frac{N-n}{N-1})\cdot n\cdot \frac{M}{N}\cdot (1-\frac{M}{N})$$

# Examples

### Exercise 36 (page 116)
Let $X$ be the damage incurred (in dollars) in a certain type of accident during a given year. Possible $X$ values are 0, 1000, 5000, 10000, with probabilities of 0.8, 0.1, 0.08, 0.02, respectively. A particular company offers a 500 dollars deductible policy. If the company wishes its expected profit to be 100 dollars, what premium amount should it charge?  

X = Damage incurred 

Y = h(x) = Amount the company pays = x - 500

```{r, echo = F}
x <- c(0, 1000, 5000, 10000)
p.x <- c(0.8, 0.1, 0.08, 0.02)
p.y <- c(0, 500, 4500, 9500)
damage.payout <- data.frame("x" = x, "p.x" = p.x, "p.y" = p.y)
damage.payout_new <- as.data.frame(t(damage.payout))
options(scipen = 50) # get rid of e's
kable(damage.payout_new, col.names = NULL) %>%
  kable_paper()
```

E[Y]=0(0.8) + 500(0.1) + 4500(0.08) + 9500(0.02) = 600 dollars

### Exercise 32 (page 16)
A certian brand of upright freezer is available in three different rated capacities: $16ft^3$, $18ft^3$, and $20ft^3$. Let $X=$ the rated capacity of a freezer of this brand sold at a certain store. Suppose that $X$ has pmf: 
```{r, echo=F}
x <- c(16,18,20)
p.x <- c(0.2,0.5,0.3)
df <- data.frame("x" = x, "p.x" = p.x)
df.new <- as.data.frame(t(df))
options(scipen = 50)
kable(df.new, col.names = NULL) %>%
  kable_paper()
```
a. **Compute $E(X)$, $E(X^2)$, and $V(X)$.**

E[x] = 16(0.2) + 18(0.5) + 20(0.3) = 3.2 + 9 + 6 = 18.2

E[$x^2$] = $16^2$(0.2) + $18^2$(0.5) + $20^2$(0.3) = 333.2

$\sigma ^2$ = Var(x) = E[$x^2$]-$[E[x]]^2$ = 333.2 - $18.2)^2$ = 1.96

$\sigma$= 1.4

b. **If the price of a freezer having capacity of $X$ is $70X-650$, what is the expected price paid by the next customer to buy a freezer?** 

c. **What is the variance of the price paid by the next custormer?** 

d. **Suppose that although the rated capacity of a freezer is $X$, the actual capacity is $h(X)=X-0.008X^2$. What is the expected actual capacity of the freezer purchased by the next customer?** 


### In Class Example (7-15-2021)
Suppose that the probaility of giving birth to a femal is $\frac{1}{2}$. A family decides to have children until they have a son. What is the probability of giving birth to 3 females before a son is born? 

$x$ = Number of females until a son is born

$x$~Geometric distribution with $1=\frac{1}{2}$

$P[x=3]=( \frac{1}{2})^2( \frac{1}{2})=\frac{1}{16}$

### Exercise 80 (page 135)
Let $X$ be the number of material anomalies occuring in a particulat region of an aircraft gas-turbine disk. The article **"Methodology for Probabilistic Life Prediction of Multiple-Anomaly Materials" (Amer. Inst. of Aeronautics and Astronautics J., 2006: 787-793)** proposes a Poisson distribution for X. Suppose that $\mu =4$. 

a. **Compute both $P(X\leq4)$ and $P(X<4)$.**

Using a TI-83 : [2nd] $\rightarrow$ [DISTR] $\rightarrow$  [C:poissoncdf(4,4)]

$P(X\leq4) = 0.6288$

Using a TI-83 : [2nd] $\rightarrow$ [DISTR] $\rightarrow$  [C:poissoncdf(4,3)]

$P(X<4)=P(X\leq 3) = 0.4335$

Note: $P(X=4) = 0.629-0.433 = 0.196$

b. **Compute $P(4\leq X \leq 8)$.**

$P(4\leq X \leq 8)= 0.979 - 0.4335 = 0.5455$

c. **Computer $P(8\leq X)$.**

Using a TI-83 : [2nd] $\rightarrow$ [DISTR] $\rightarrow$  [C:poissoncdf(4,8)]

$P(8\leq X) = 0.979$

d. **What is the probability that the numebr of anomalies exceeds its mean value by no more than one standard deviation?** 

$P[X>\mu + \sigma]=P[X>4+2]=P[x>6]=1-F(5)=0.21486$

### Exercise 87 (page 136)
The number of requests for assitance received by a towing service is a Poisson process with rate $\alpha =4$ per hour. 

a. **Compute the probability that exactly ten requests are received during a particular 2-hour period.** 

Using a TI-83 : [2nd] $\rightarrow$ [DISTR] $\rightarrow$  [C:poissonpdf(8,10)]

$P(X=10)=0.09926$

b. **If the operators of the towing service take a 30-min break for lunch, what is the probability that they do not miss any calls for assistance?** 

Define Y as the number of requests during a 30 minute period.

$y$~$P(\mu =\frac{4}{2}=2)$

Using a TI-83 : [2nd] $\rightarrow$ [DISTR] $\rightarrow$  [C:poissonpdf(2,0)]

$P(Y=0)=0.13533$

c. **How many calls would you expect during their break?** 

$E(Y)=2$

### Exercise 50 (page 124)
A particular telephone number is used to receive both voice calls and fax messages. Suppose that 25% of the incoming calls involve fax messages, and consider a sample of 25 incoming calls. What is the probability that

a. **At most 6 calls involve a fax message?**

[binomialcdf(25,.25,6)]=0.561

b. **Exactly 6 of the calls involve a fax message?**

$P[x=6]$ = $25\choose6$ $(0.25)^6(0.75)^{19}$

[binomialpdf(25,.25,6)]=0.1828

c. **At least 6 of the calls involve a fax message?**

[binomialpdf(25,.25,5)]=0.1828

$P(x\geq 6)=1-P[x<6]=1-P[x\leq 5]=0.6217$

d. **More than 6 of the calls involve a fax message?**

$P[x\geq 6]=1-0.5610=0.439$
